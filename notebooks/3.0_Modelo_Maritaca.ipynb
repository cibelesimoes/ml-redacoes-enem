{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch\n",
    "# !pip install transformers[torch]\n",
    "#!pip uninstall numpy\n",
    "#!pip install numpy==1.26.4\n",
    "#!pip install --upgrade transformers\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelWithLMHead, Trainer, TrainingArguments\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Deve retornar True\n",
    "print(torch.cuda.get_device_name(0))  # Nome da sua GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando GPU: NVIDIA GeForce RTX 4060\n",
      "tensor([[-1.4728, -1.4469,  1.0046],\n",
      "        [ 0.4678,  1.3486, -1.6028],\n",
      "        [ 0.0810,  0.6651,  0.0275]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Verificar se a GPU está disponível\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Define a GPU como dispositivo\n",
    "    print(f\"Usando GPU: {torch.cuda.get_device_name(0)}\")  # Imprime o nome da GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Se não houver GPU disponível, usa a CPU\n",
    "    print(\"Usando CPU\")\n",
    "\n",
    "# Exemplo de como mover um tensor para a GPU\n",
    "tensor = torch.randn(3, 3)  # Tensor aleatório\n",
    "tensor = tensor.to(device)  # Mover para o dispositivo correto (GPU ou CPU)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['nvidia-smi'], returncode=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Verificar o uso da GPU\n",
    "subprocess.run([\"nvidia-smi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tema</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>comments</th>\n",
       "      <th>criteria_score_1</th>\n",
       "      <th>criteria_score_2</th>\n",
       "      <th>criteria_score_3</th>\n",
       "      <th>criteria_score_4</th>\n",
       "      <th>criteria_score_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Os ursos polares da Rússia e um dilema ecológico</td>\n",
       "      <td>As mudanças que afetam o clima de nosso planet...</td>\n",
       "      <td>Natureza caótica</td>\n",
       "      <td>A natureza possui um equilíbrio singular singu...</td>\n",
       "      <td>Texto muito bom, com alguns erros e equívocos ...</td>\n",
       "      <td>160</td>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Os ursos polares da Rússia e um dilema ecológico</td>\n",
       "      <td>As mudanças que afetam o clima de nosso planet...</td>\n",
       "      <td>Em busca de um novo lar</td>\n",
       "      <td>Com o aumento do aquecimento global, várias es...</td>\n",
       "      <td>Texto mediano. Além dos problemas pontuais neg...</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Os ursos polares da Rússia e um dilema ecológico</td>\n",
       "      <td>As mudanças que afetam o clima de nosso planet...</td>\n",
       "      <td>A ação antrópica e suas consequências</td>\n",
       "      <td>Há, na pré-história, pinturas rupestres que re...</td>\n",
       "      <td>Texto prolixo e repetitivo, com problemas pont...</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Os ursos polares da Rússia e um dilema ecológico</td>\n",
       "      <td>As mudanças que afetam o clima de nosso planet...</td>\n",
       "      <td>O aquecimento global e os ursos</td>\n",
       "      <td>O aquecimento global vem afetando o mundo nos ...</td>\n",
       "      <td>Lamentavelmente, o texto é fraco, insuficiente...</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Os ursos polares da Rússia e um dilema ecológico</td>\n",
       "      <td>As mudanças que afetam o clima de nosso planet...</td>\n",
       "      <td>As consequências do degelo no Ártico</td>\n",
       "      <td>O aumento do aquecimento global provocou, no í...</td>\n",
       "      <td>Texto mediano. O autor cumpre as exigências da...</td>\n",
       "      <td>80</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tema  \\\n",
       "0  Os ursos polares da Rússia e um dilema ecológico   \n",
       "1  Os ursos polares da Rússia e um dilema ecológico   \n",
       "2  Os ursos polares da Rússia e um dilema ecológico   \n",
       "3  Os ursos polares da Rússia e um dilema ecológico   \n",
       "4  Os ursos polares da Rússia e um dilema ecológico   \n",
       "\n",
       "                                         description  \\\n",
       "0  As mudanças que afetam o clima de nosso planet...   \n",
       "1  As mudanças que afetam o clima de nosso planet...   \n",
       "2  As mudanças que afetam o clima de nosso planet...   \n",
       "3  As mudanças que afetam o clima de nosso planet...   \n",
       "4  As mudanças que afetam o clima de nosso planet...   \n",
       "\n",
       "                                   title  \\\n",
       "0                       Natureza caótica   \n",
       "1                Em busca de um novo lar   \n",
       "2  A ação antrópica e suas consequências   \n",
       "3        O aquecimento global e os ursos   \n",
       "4   As consequências do degelo no Ártico   \n",
       "\n",
       "                                                text  \\\n",
       "0  A natureza possui um equilíbrio singular singu...   \n",
       "1  Com o aumento do aquecimento global, várias es...   \n",
       "2  Há, na pré-história, pinturas rupestres que re...   \n",
       "3  O aquecimento global vem afetando o mundo nos ...   \n",
       "4  O aumento do aquecimento global provocou, no í...   \n",
       "\n",
       "                                            comments  criteria_score_1  \\\n",
       "0  Texto muito bom, com alguns erros e equívocos ...               160   \n",
       "1  Texto mediano. Além dos problemas pontuais neg...               120   \n",
       "2  Texto prolixo e repetitivo, com problemas pont...               120   \n",
       "3  Lamentavelmente, o texto é fraco, insuficiente...               120   \n",
       "4  Texto mediano. O autor cumpre as exigências da...                80   \n",
       "\n",
       "   criteria_score_2  criteria_score_3  criteria_score_4  criteria_score_5  \n",
       "0               200               160               200               160  \n",
       "1               120               120               120               120  \n",
       "2               120                80               120               120  \n",
       "3                80                80                80                80  \n",
       "4               120               120               120               120  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caminho_redacoes_corrigidas = os.path.join('..', 'data', 'redacoes_corrigidas_uol_fora_enem.csv')\n",
    "\n",
    "df_redacoes_corrigidas = pd.read_csv(caminho_redacoes_corrigidas, sep=\";\")\n",
    "df_redacoes_corrigidas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redacoes_corrigidas[\"nota_final\"] = df_redacoes_corrigidas[\"criteria_score_1\"] + df_redacoes_corrigidas[\"criteria_score_2\"] + df_redacoes_corrigidas[\"criteria_score_3\"] + df_redacoes_corrigidas[\"criteria_score_4\"] + df_redacoes_corrigidas[\"criteria_score_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Os ursos polares da Rússia e um dilema ecológico'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_redacoes_corrigidas['tema'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Você é um estudante do ensino médio que está prestando vestibular para o ENEM, o Exame Nacional do Ensino Médio. \n",
      "    Você deve se atentar à \n",
      "     - Domínio da norma-padrão da língua escrita; \n",
      "     - Compreensão do tema; \n",
      "     - Organização das informações e argumentos;\n",
      "     - Correta aplicação da lógica;\n",
      "    - Apresentar uma proposta de intervenção para o problema.\n",
      "    \n",
      "    O tema da redaçãao neste ano é \"Os ursos polares da Rússia e um dilema ecológico\". \n",
      "    E você escreveu a seguinte redação:\n",
      "\n",
      "    A natureza possui um equilíbrio singular singular, de modo que todos os seres vivos desempenham uma função - os ursos polares, por exemplo, ao se alimentarem de peixes e focas focas, regulam a quantidade de indivíduos nessas populações. Contudo, os homens ainda não compreendem a importância do respeito ao meio ambiente para a preservação da própria espécie e, por isso, têm arcado com as consequências, como o ataque dos ursos polares a comunidades humanas por diminuição da oferta de alimentos, devido ao derretimento das geleiras. A preocupação com a preservação ambiental tornou-se uma pauta mais frequente urgente, a partir da década de 70, desde então 70. Desde então, vários acordos para diminuir as queimadas, o desmatamento e, principalmente, a emissão de gases poluentes foram assinados, mas não atingiram o efeito esperado, pois demandam uma mudança radical na economia, que parte dos países não está disposta a fazer. Como o Estados Unidos, segundo maior poluidor do mundo, que não participou do Tratado de Kyoto e se retirou do Acordo de Paris devido ao comprometimento econômico que eles acarretariam. Um dos impasses resultantes das mudanças climáticas é a maneira de lidar com animais que invadem as cidades, já que é necessário um cuidado maior para que as pessoas e os animais não se machuquem. O caso dos ursos polares é preocupante nesse aspecto, pois eles são violentos e precisam ser contidos depressa, mas, como estão em extinção, não podem ser mortos e, mesmo que pudessem, isso não resolveria o problema e ainda originaria outros desequilíbrios. Para efetivamente saná-lo, deve-se mitigar a causa, que é o aquecimento global através da redução do uso de combustíveis fosseis fósseis. Enquanto não há priorização desse assunto nas ações governamentais, a natureza e a sociedade, em geral, continuam sofrendo. É por essa razão que a população mundial precisa se mobilizar e valorizar politicas políticas sustentáveis dado que o planeta é responsabilidade de todos e adiar a sua preservação pode trazer resultados irreparáveis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Passo 2: Pré-processar os dados\n",
    "# Unir a redação e correção no formato de entrada do modelo\n",
    "\n",
    "prompt_input = \"\"\"\n",
    "    Você é um estudante do ensino médio que está prestando vestibular para o ENEM, o Exame Nacional do Ensino Médio. \n",
    "    Você deve se atentar à \n",
    "     - Domínio da norma-padrão da língua escrita; \n",
    "     - Compreensão do tema; \n",
    "     - Organização das informações e argumentos;\n",
    "     - Correta aplicação da lógica;\n",
    "    - Apresentar uma proposta de intervenção para o problema.\n",
    "    \n",
    "    O tema da redaçãao neste ano é \"{}\". \n",
    "    E você escreveu a seguinte redação:\n",
    "\n",
    "    {}\n",
    "\"\"\".format(df_redacoes_corrigidas['tema'][0],  df_redacoes_corrigidas['text'][0])\n",
    "\n",
    "print(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   A correção da sua redação foi a seguinte:\n",
      "   Texto muito bom, com alguns erros e equívocos em termos de conteúdo, que impedem a atribuição de uma nota ainda melhor. Parágrafo 1) Texto muito bom em termos de linguagem. Os erros gramaticais apontados em verde impedem que se lhe atribua a nota máxima. Destaque-se que o título não tem nada que ver com o texto. Parágrafo 2) O autor compreendeu o tema e desenvolveu um texto dissertativo-argumentativo obedecendo as exigências desta competência. Parágrafo 3) Argumentação boa. O autor contextualiza bem o problema. Só se equivoca ao dizer que os homens não compreendem a necessidade de respeitar o meio ambiente. A afirmação é excessivamente genérica e contraditória, pois, se vários acordos e protocolos foram assinados desde os anos 70, uma parte significativa das nações se mostra preocupada com o problema. Também é estranho argumentar com o \"segundo maior poluidor\". Seria melhor apontar o primeiro maior ou os dois países. Parágrafo 4) A argumentação está bem construída e o texto é coerente e coeso. Parágrafo 5) A sugestão é genérica e está voltada para o problema mais amplo do aquecimento global. Como ficam os ursos?. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_output = \"\"\"\n",
    "   A correção da sua redação foi a seguinte:\n",
    "   {}. \n",
    "\"\"\".format(df_redacoes_corrigidas['comments'][0])\n",
    "\n",
    "print(prompt_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_output = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output'],\n",
       "    num_rows: 3726\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input_output['input'] = df_redacoes_corrigidas.apply(\n",
    "    lambda x: prompt_input.format(x['tema'], x['text']), axis=1\n",
    ")\n",
    "df_input_output['output'] = df_redacoes_corrigidas.apply(\n",
    "    lambda x: prompt_output.format(x['comments']), axis=1\n",
    ")\n",
    "\n",
    "train_data = Dataset.from_pandas(df_input_output[['input', 'output']])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(\"maritaca-ai/sabia-7b\")\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     \"maritaca-ai/sabia-7b\",\n",
    "#     device_map=\"auto\",  # Automatically loads the model in the GPU, if there is one. Requires pip install acelerate\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.bfloat16   # If your GPU does not support bfloat16, change to torch.float16\n",
    "# )  \n",
    "\n",
    "# prompt = \"\"\"Classifique a resenha de filme como \"positiva\" ou \"negativa\".\n",
    "\n",
    "# Resenha: Gostei muito do filme, é o melhor do ano!\n",
    "# Classe: positiva\n",
    "\n",
    "# Resenha: O filme deixa muito a desejar.\n",
    "# Classe: negativa\n",
    "\n",
    "# Resenha: Apesar de longo, valeu o ingresso.\n",
    "# Classe:\"\"\"\n",
    "\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# output = model.generate(\n",
    "#     input_ids[\"input_ids\"].to(\"cuda\"),\n",
    "#     max_length=1024,\n",
    "#     eos_token_id=tokenizer.encode(\"\\n\"))  # Stop generation when a \"\\n\" token is dectected\n",
    "\n",
    "# # The output contains the input tokens, so we have to skip them.\n",
    "# output = output[0][len(input_ids[\"input_ids\"][0]):]\n",
    "\n",
    "# print(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# Carregando o tokenizer e o modelo pré-treinado\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"maritaca-ai/sabia-7b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"maritaca-ai/sabia-7b\",\n",
    "    device_map=None,  # Desativa o gerenciamento automático de dispositivos\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16  # Certifique-se de que sua GPU suporta esse tipo\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"Giuliano/llama-2-7b-periquito-pt-br\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Giuliano/llama-2-7b-periquito-pt-br\",\n",
    "#     device_map=\"auto\",\n",
    "#     offload_folder=\"./offload\",  # Move partes do modelo para o disco\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Carregando o tokenizer e o modelo pré-treinado\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(\"maritaca-ai/sabia-2-tokenizer-small\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     \"maritaca-ai/sabia-2-tokenizer-small\",\n",
    "#     device_map=\"auto\",\n",
    "#     offload_folder=\"./offload\",  # Move partes do modelo para o disco\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.float16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3726/3726 [00:10<00:00, 346.21 examples/s]\n",
      "c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\cibel\\AppData\\Local\\Temp\\ipykernel_26704\\1586421316.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Criando o dataset compatível com Hugging Face\n",
    "train_data = Dataset.from_pandas(df_input_output[['input', 'output']])\n",
    "\n",
    "# Tokenizando os dados\n",
    "def preprocess_data(batch):\n",
    "    inputs = tokenizer(batch['input'], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    outputs = tokenizer(batch['output'], max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Para o modelo de causal LM, deslocamos os labels\n",
    "    labels = outputs['input_ids']\n",
    "    inputs['labels'] = labels\n",
    "    return inputs\n",
    "\n",
    "tokenized_data = train_data.map(preprocess_data, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "\n",
    "# **Treinando o modelo**\n",
    "# Configuração de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\", #\"epoch\"\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,  # Defina como True se desejar fazer upload para o Hugging Face Hub\n",
    "    fp16=True  # Se a GPU suportar\n",
    ")\n",
    "\n",
    "# Criando o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5589 [00:00<?, ?it/s]c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 260.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Iniciando o treinamento\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:2242\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2242\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 260.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Iniciando o treinamento\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo treinado\n",
    "model.save_pretrained(\"./trained_model_maritaca_v1\")\n",
    "tokenizer.save_pretrained(\"./trained_model_maritaca_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar o modelo e o tokenizer ajustados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "def testar_modelo(prompt, modelo_caminho=\"./trained_model\", max_length=512, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Método para testar o modelo treinado.\n",
    "    \n",
    "    Parâmetros:\n",
    "        - prompt (str): O texto de entrada para o modelo.\n",
    "        - modelo_caminho (str): Caminho para o modelo treinado salvo.\n",
    "        - max_length (int): Comprimento máximo da geração.\n",
    "        - device (str): Dispositivo a ser usado ('cuda' para GPU, 'cpu' para CPU).\n",
    "    \n",
    "    Retorno:\n",
    "        - output_text (str): O texto gerado pelo modelo.\n",
    "    \"\"\"\n",
    "    # Carregar o modelo treinado e o tokenizer\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(modelo_caminho)\n",
    "    model = LlamaForCausalLM.from_pretrained(modelo_caminho)\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenizar o prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Gerar texto com o modelo\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        num_return_sequences=1,  # Gerar apenas uma saída\n",
    "        temperature=0.7,         # Controlar aleatoriedade\n",
    "        top_k=50                 # Limitar escolhas de palavras\n",
    "    )\n",
    "\n",
    "    # Decodificar o texto gerado\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Retornar apenas a parte gerada\n",
    "    return output_text[len(prompt):].strip()\n",
    "\n",
    "# Exemplo de uso\n",
    "prompt_teste = \"\"\"Tema: Tecnologia na educação\n",
    "Redação: A tecnologia tem transformado a maneira como aprendemos. Desde o uso de computadores em sala de aula até ferramentas como inteligência artificial, os avanços tecnológicos têm ampliado o acesso à educação. Contudo, é necessário que haja um equilíbrio entre inovação e inclusão digital.\n",
    "\n",
    "Correção:\"\"\"\n",
    "\n",
    "resultado = testar_modelo(prompt_teste)\n",
    "print(\"Saída do modelo:\", resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar o modelo e o tokenizer ajustados\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./modelo_finetuned_enem_v3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./modelo_finetuned_enem_v3\")\n",
    "\n",
    "# Definir o dispositivo (GPU se disponível)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Função para avaliar redação\n",
    "# def avaliar_redacao(redacao, tema, max_length = 750):\n",
    "#     \"\"\"\n",
    "#     Avalia uma redação baseada em um tema usando um modelo fine-tuned GPT-like.\n",
    "\n",
    "#     Args:\n",
    "#         redacao (str): O texto da redação.\n",
    "#         tema (str): O tema da redação.\n",
    "#         max_length (int): Comprimento máximo da resposta gerada.\n",
    "\n",
    "#     Returns:\n",
    "#         str: Resposta gerada pelo modelo.\n",
    "#     \"\"\"\n",
    "#     # Criar o prompt com a redação e o tema\n",
    "#     prompt_input = f\"\"\"\n",
    "#     Você é um estudante do ensino médio que está prestando vestibular para o ENEM, o Exame Nacional do Ensino Médio.\n",
    "#     Você deve se atentar à:\n",
    "#     - Domínio da norma-padrão da língua escrita;\n",
    "#     - Compreensão do tema;\n",
    "#     - Organização das informações e argumentos;\n",
    "#     - Correta aplicação da lógica;\n",
    "#     - Apresentar uma proposta de intervenção para o problema.\n",
    "\n",
    "#     O tema da redação neste ano é \"{tema}\".\n",
    "#     E você escreveu a seguinte redação:\n",
    "\n",
    "#     {redacao}\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Tokenizar o prompt usando o tokenizer e mover os tensores para o dispositivo correto\n",
    "#     inputs = tokenizer(\n",
    "#         prompt_input,\n",
    "#         return_tensors=\"pt\",  # Gera tensores PyTorch\n",
    "#         truncation=True # Limita o comprimento do prompt\n",
    "#     )\n",
    "#     input_ids = inputs[\"input_ids\"].to(device)\n",
    "#     attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "#     # Gerar a saída do modelo\n",
    "#     outputs = model.generate(\n",
    "#         input_ids=input_ids,  # IDs tokenizados\n",
    "#         attention_mask=attention_mask,  # Máscara de atenção\n",
    "#         max_length=max_length,  # Comprimento máximo do texto gerado\n",
    "#         num_return_sequences=1,  # Gera apenas uma sequência\n",
    "#         do_sample=True,  # Sampling para maior diversidade\n",
    "#         top_p=0.95,  # Nucleus sampling\n",
    "#         top_k=50  # Limita o número de tokens considerados na geração\n",
    "#     )\n",
    "    \n",
    "#     # Decodificar a resposta do modelo\n",
    "#     resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "#     return resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_redacao(redacao, tema, max_length = 750):\n",
    "    # Criar o prompt com a redação e o tema\n",
    "    \n",
    "    prompt_input_ = \"\"\"\n",
    "        Você é um estudante do ensino médio que está prestando vestibular para o ENEM, o Exame Nacional do Ensino Médio. \n",
    "        Você deve se atentar à \n",
    "        - Domínio da norma-padrão da língua escrita; \n",
    "        - Compreensão do tema; \n",
    "        - Organização das informações e argumentos;\n",
    "        - Correta aplicação da lógica;\n",
    "        - Apresentar uma proposta de intervenção para o problema.\n",
    "        \n",
    "        O tema da redaçãao neste ano é \"{}\". \n",
    "        E você escreveu a seguinte redação:\n",
    "\n",
    "        {}\n",
    "    \"\"\".format(tema, redacao)\n",
    "    \n",
    "    prompt_input = \"Oi, tudo bem?\"\n",
    "\n",
    "    # Tokenizar o prompt\n",
    "    inputs = tokenizer(prompt_input, \n",
    "                            return_tensors=\"pt\"\n",
    "                            ).to(device)\n",
    "    #inputs = tokenizer(prompt, return_tensors=\"pt\").to(device) - # PT é de temnsorflow = TODO: Usar modelos, maritaca, sabias, caramelo\n",
    "    \n",
    "    # Gerar a saída do modelo\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, num_return_sequences=1, do_sample=True, top_p=0.95, top_k=50)\n",
    "    \n",
    "    # Decodificar a resposta do modelo\n",
    "    resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oi, tudo bem? se*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".**teen?)*teen?\".*teen?\".teen?\".*teen?\".teen?\".*teen?\".teen?\".*teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de uso\n",
    "redacao_exemplo = \"\"\"\n",
    "A imigração de refugiado tem sido um dos maiores desafios das nações. Este transpassa a questão humanitária e vai \n",
    "de encontro a  soberania de muitos países. Não diferente, O  Brasil enfrenta, como os demais  situação parecida. \n",
    "Nos últimos anos uma grande quantidade de refugiados têm transpassada  a fronteira do país forçando o poder \n",
    "público a tomar medidas que garantam a integridade da soberania nacional e que esteja em consonância aos direitos humanos.  \n",
    "Várias nações do mundo estão passando por percalços, seja por  guerras, crises econômicas, conflitos internos, regimes ditatoriais... \n",
    "Fato que  faz surgir refugiados que vislumbram nos países mais próximos a salva guarda  para manterem a integridade. O que para estes  \n",
    "é um alento, para o país em que vão se refugiam  se torna um problema. Que muitas vezes vezes,  não está relacionado diretamente \n",
    "a um sentimento xenófobo, mas sim por  uma questão social. No caso do Brasil, os refugiados da nossa nação adjacente se abrigaram \n",
    "em uma cidade de um estado da federação  que tem grande dificuldade para cuidar até mesmo dos nativos. Causando  um verdadeiro caos. \n",
    "O que  fez aumentar ainda mais o sofrimento de quem já está em uma situação extrema. Destarte que  não basta o mero acolhimento, \n",
    "deve-se socorrer e fomentar uma nova expectativa de vida nestas  pessoas que estão distantes do país de origem. O que  tende a \n",
    "demandar tempo e recursos. No caso brasileiro  um dispêndio a mais em um país que está em busca de reduzir despesas. Salientando \n",
    "que se faz necessário uma previa  triagem para evitar que pessoas criminosas e mal-intencionadas venham fazer da nação um local \n",
    "para praticas criminosa  e potencialize a violência. Desprende  que o maior desafio no trato com os imigrantes está em conciliar \n",
    "a soberana força nacional de proteção com a aplicação de direitos humanos. É impensável deixar de ajudar o semelhante em apuros. \n",
    "Efetivar esse apoio sem prejudicar os demais é tarefa que requer recursos e planejamentos. O que aos poucos o país vai se consolidando.\n",
    "\n",
    "\"\"\"\n",
    "tema_exemplo = \"Desafios na imigração de refugiados\"\n",
    "\n",
    "correcao = avaliar_redacao(redacao_exemplo, tema_exemplo)\n",
    "print(correcao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
