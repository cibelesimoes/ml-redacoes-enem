{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch\n",
    "# !pip install transformers[torch]\n",
    "#!pip uninstall numpy\n",
    "#!pip install numpy==1.26.4\n",
    "#!pip install --upgrade transformers\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelWithLMHead, Trainer, TrainingArguments\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Deve retornar True\n",
    "print(torch.cuda.get_device_name(0))  # Nome da sua GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando GPU: NVIDIA GeForce RTX 4060\n",
      "tensor([[-1.5453,  0.2253,  0.6028],\n",
      "        [-0.3446, -1.7721,  0.7371],\n",
      "        [-0.2583, -0.8747, -0.9884]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Verificar se a GPU est√° dispon√≠vel\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Define a GPU como dispositivo\n",
    "    print(f\"Usando GPU: {torch.cuda.get_device_name(0)}\")  # Imprime o nome da GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Se n√£o houver GPU dispon√≠vel, usa a CPU\n",
    "    print(\"Usando CPU\")\n",
    "\n",
    "# Exemplo de como mover um tensor para a GPU\n",
    "tensor = torch.randn(3, 3)  # Tensor aleat√≥rio\n",
    "tensor = tensor.to(device)  # Mover para o dispositivo correto (GPU ou CPU)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['nvidia-smi'], returncode=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Verificar o uso da GPU\n",
    "subprocess.run([\"nvidia-smi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tema</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>comments</th>\n",
       "      <th>criteria_score_1</th>\n",
       "      <th>criteria_score_2</th>\n",
       "      <th>criteria_score_3</th>\n",
       "      <th>criteria_score_4</th>\n",
       "      <th>criteria_score_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Os ursos polares da R√∫ssia e um dilema ecol√≥gico</td>\n",
       "      <td>As mudan√ßas que afetam o clima de nosso planet...</td>\n",
       "      <td>Natureza ca√≥tica</td>\n",
       "      <td>A natureza possui um equil√≠brio singular singu...</td>\n",
       "      <td>Texto muito bom, com alguns erros e equ√≠vocos ...</td>\n",
       "      <td>160</td>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Os ursos polares da R√∫ssia e um dilema ecol√≥gico</td>\n",
       "      <td>As mudan√ßas que afetam o clima de nosso planet...</td>\n",
       "      <td>Em busca de um novo lar</td>\n",
       "      <td>Com o aumento do aquecimento global, v√°rias es...</td>\n",
       "      <td>Texto mediano. Al√©m dos problemas pontuais neg...</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Os ursos polares da R√∫ssia e um dilema ecol√≥gico</td>\n",
       "      <td>As mudan√ßas que afetam o clima de nosso planet...</td>\n",
       "      <td>A a√ß√£o antr√≥pica e suas consequ√™ncias</td>\n",
       "      <td>H√°, na pr√©-hist√≥ria, pinturas rupestres que re...</td>\n",
       "      <td>Texto prolixo e repetitivo, com problemas pont...</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Os ursos polares da R√∫ssia e um dilema ecol√≥gico</td>\n",
       "      <td>As mudan√ßas que afetam o clima de nosso planet...</td>\n",
       "      <td>O aquecimento global e os ursos</td>\n",
       "      <td>O aquecimento global vem afetando o mundo nos ...</td>\n",
       "      <td>Lamentavelmente, o texto √© fraco, insuficiente...</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Os ursos polares da R√∫ssia e um dilema ecol√≥gico</td>\n",
       "      <td>As mudan√ßas que afetam o clima de nosso planet...</td>\n",
       "      <td>As consequ√™ncias do degelo no √Årtico</td>\n",
       "      <td>O aumento do aquecimento global provocou, no √≠...</td>\n",
       "      <td>Texto mediano. O autor cumpre as exig√™ncias da...</td>\n",
       "      <td>80</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tema  \\\n",
       "0  Os ursos polares da R√∫ssia e um dilema ecol√≥gico   \n",
       "1  Os ursos polares da R√∫ssia e um dilema ecol√≥gico   \n",
       "2  Os ursos polares da R√∫ssia e um dilema ecol√≥gico   \n",
       "3  Os ursos polares da R√∫ssia e um dilema ecol√≥gico   \n",
       "4  Os ursos polares da R√∫ssia e um dilema ecol√≥gico   \n",
       "\n",
       "                                         description  \\\n",
       "0  As mudan√ßas que afetam o clima de nosso planet...   \n",
       "1  As mudan√ßas que afetam o clima de nosso planet...   \n",
       "2  As mudan√ßas que afetam o clima de nosso planet...   \n",
       "3  As mudan√ßas que afetam o clima de nosso planet...   \n",
       "4  As mudan√ßas que afetam o clima de nosso planet...   \n",
       "\n",
       "                                   title  \\\n",
       "0                       Natureza ca√≥tica   \n",
       "1                Em busca de um novo lar   \n",
       "2  A a√ß√£o antr√≥pica e suas consequ√™ncias   \n",
       "3        O aquecimento global e os ursos   \n",
       "4   As consequ√™ncias do degelo no √Årtico   \n",
       "\n",
       "                                                text  \\\n",
       "0  A natureza possui um equil√≠brio singular singu...   \n",
       "1  Com o aumento do aquecimento global, v√°rias es...   \n",
       "2  H√°, na pr√©-hist√≥ria, pinturas rupestres que re...   \n",
       "3  O aquecimento global vem afetando o mundo nos ...   \n",
       "4  O aumento do aquecimento global provocou, no √≠...   \n",
       "\n",
       "                                            comments  criteria_score_1  \\\n",
       "0  Texto muito bom, com alguns erros e equ√≠vocos ...               160   \n",
       "1  Texto mediano. Al√©m dos problemas pontuais neg...               120   \n",
       "2  Texto prolixo e repetitivo, com problemas pont...               120   \n",
       "3  Lamentavelmente, o texto √© fraco, insuficiente...               120   \n",
       "4  Texto mediano. O autor cumpre as exig√™ncias da...                80   \n",
       "\n",
       "   criteria_score_2  criteria_score_3  criteria_score_4  criteria_score_5  \n",
       "0               200               160               200               160  \n",
       "1               120               120               120               120  \n",
       "2               120                80               120               120  \n",
       "3                80                80                80                80  \n",
       "4               120               120               120               120  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caminho_redacoes_corrigidas = os.path.join('..', 'data', 'redacoes_corrigidas_uol_fora_enem.csv')\n",
    "\n",
    "df_redacoes_corrigidas = pd.read_csv(caminho_redacoes_corrigidas, sep=\";\")\n",
    "df_redacoes_corrigidas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redacoes_corrigidas[\"nota_final\"] = df_redacoes_corrigidas[\"criteria_score_1\"] + df_redacoes_corrigidas[\"criteria_score_2\"] + df_redacoes_corrigidas[\"criteria_score_3\"] + df_redacoes_corrigidas[\"criteria_score_4\"] + df_redacoes_corrigidas[\"criteria_score_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Os ursos polares da R√∫ssia e um dilema ecol√≥gico'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_redacoes_corrigidas['tema'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Voc√™ √© um estudante do ensino m√©dio que est√° prestando vestibular para o ENEM, o Exame Nacional do Ensino M√©dio. \n",
      "    Voc√™ deve se atentar √† \n",
      "     - Dom√≠nio da norma-padr√£o da l√≠ngua escrita; \n",
      "     - Compreens√£o do tema; \n",
      "     - Organiza√ß√£o das informa√ß√µes e argumentos;\n",
      "     - Correta aplica√ß√£o da l√≥gica;\n",
      "    - Apresentar uma proposta de interven√ß√£o para o problema.\n",
      "    \n",
      "    O tema da reda√ß√£ao neste ano √© \"Os ursos polares da R√∫ssia e um dilema ecol√≥gico\". \n",
      "    E voc√™ escreveu a seguinte reda√ß√£o:\n",
      "\n",
      "    A natureza possui um equil√≠brio singular singular, de modo que todos os seres vivos desempenham uma fun√ß√£o - os ursos polares, por exemplo, ao se alimentarem de peixes e focas focas, regulam a quantidade de indiv√≠duos nessas popula√ß√µes. Contudo, os homens ainda n√£o compreendem a import√¢ncia do respeito ao meio ambiente para a preserva√ß√£o da pr√≥pria esp√©cie e, por isso, t√™m arcado com as consequ√™ncias, como o ataque dos ursos polares a comunidades humanas por diminui√ß√£o da oferta de alimentos, devido ao derretimento das geleiras. A preocupa√ß√£o com a preserva√ß√£o ambiental tornou-se uma pauta mais frequente urgente, a partir da d√©cada de 70, desde ent√£o 70. Desde ent√£o, v√°rios acordos para diminuir as queimadas, o desmatamento e, principalmente, a emiss√£o de gases poluentes foram assinados, mas n√£o atingiram o efeito esperado, pois demandam uma mudan√ßa radical na economia, que parte dos pa√≠ses n√£o est√° disposta a fazer. Como o Estados Unidos, segundo maior poluidor do mundo, que n√£o participou do Tratado de Kyoto e se retirou do Acordo de Paris devido ao comprometimento econ√¥mico que eles acarretariam. Um dos impasses resultantes das mudan√ßas clim√°ticas √© a maneira de lidar com animais que invadem as cidades, j√° que √© necess√°rio um cuidado maior para que as pessoas e os animais n√£o se machuquem. O caso dos ursos polares √© preocupante nesse aspecto, pois eles s√£o violentos e precisam ser contidos depressa, mas, como est√£o em extin√ß√£o, n√£o podem ser mortos e, mesmo que pudessem, isso n√£o resolveria o problema e ainda originaria outros desequil√≠brios. Para efetivamente san√°-lo, deve-se mitigar a causa, que √© o aquecimento global atrav√©s da redu√ß√£o do uso de combust√≠veis fosseis f√≥sseis. Enquanto n√£o h√° prioriza√ß√£o desse assunto nas a√ß√µes governamentais, a natureza e a sociedade, em geral, continuam sofrendo. √â por essa raz√£o que a popula√ß√£o mundial precisa se mobilizar e valorizar politicas pol√≠ticas sustent√°veis dado que o planeta √© responsabilidade de todos e adiar a sua preserva√ß√£o pode trazer resultados irrepar√°veis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Passo 2: Pr√©-processar os dados\n",
    "# Unir a reda√ß√£o e corre√ß√£o no formato de entrada do modelo\n",
    "\n",
    "prompt_input = \"\"\"\n",
    "    Voc√™ √© um estudante do ensino m√©dio que est√° prestando vestibular para o ENEM, o Exame Nacional do Ensino M√©dio. \n",
    "    Voc√™ deve se atentar √† \n",
    "     - Dom√≠nio da norma-padr√£o da l√≠ngua escrita; \n",
    "     - Compreens√£o do tema; \n",
    "     - Organiza√ß√£o das informa√ß√µes e argumentos;\n",
    "     - Correta aplica√ß√£o da l√≥gica;\n",
    "    - Apresentar uma proposta de interven√ß√£o para o problema.\n",
    "    \n",
    "    O tema da reda√ß√£ao neste ano √© \"{}\". \n",
    "    E voc√™ escreveu a seguinte reda√ß√£o:\n",
    "\n",
    "    {}\n",
    "\"\"\".format(df_redacoes_corrigidas['tema'][0],  df_redacoes_corrigidas['text'][0])\n",
    "\n",
    "print(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   A corre√ß√£o da sua reda√ß√£o foi a seguinte:\n",
      "   Texto muito bom, com alguns erros e equ√≠vocos em termos de conte√∫do, que impedem a atribui√ß√£o de uma nota ainda melhor. Par√°grafo 1) Texto muito bom em termos de linguagem. Os erros gramaticais apontados em verde impedem que se lhe atribua a nota m√°xima. Destaque-se que o t√≠tulo n√£o tem nada que ver com o texto. Par√°grafo 2) O autor compreendeu o tema e desenvolveu um texto dissertativo-argumentativo obedecendo as exig√™ncias desta compet√™ncia. Par√°grafo 3) Argumenta√ß√£o boa. O autor contextualiza bem o problema. S√≥ se equivoca ao dizer que os homens n√£o compreendem a necessidade de respeitar o meio ambiente. A afirma√ß√£o √© excessivamente gen√©rica e contradit√≥ria, pois, se v√°rios acordos e protocolos foram assinados desde os anos 70, uma parte significativa das na√ß√µes se mostra preocupada com o problema. Tamb√©m √© estranho argumentar com o \"segundo maior poluidor\". Seria melhor apontar o primeiro maior ou os dois pa√≠ses. Par√°grafo 4) A argumenta√ß√£o est√° bem constru√≠da e o texto √© coerente e coeso. Par√°grafo 5) A sugest√£o √© gen√©rica e est√° voltada para o problema mais amplo do aquecimento global. Como ficam os ursos?. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_output = \"\"\"\n",
    "   A corre√ß√£o da sua reda√ß√£o foi a seguinte:\n",
    "   {}. \n",
    "\"\"\".format(df_redacoes_corrigidas['comments'][0])\n",
    "\n",
    "print(prompt_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_output = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output'],\n",
       "    num_rows: 3726\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input_output['input'] = df_redacoes_corrigidas.apply(\n",
    "    lambda x: prompt_input.format(x['tema'], x['text']), axis=1\n",
    ")\n",
    "df_input_output['output'] = df_redacoes_corrigidas.apply(\n",
    "    lambda x: prompt_output.format(x['comments']), axis=1\n",
    ")\n",
    "\n",
    "train_data = Dataset.from_pandas(df_input_output[['input', 'output']])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(\"maritaca-ai/sabia-7b\")\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     \"maritaca-ai/sabia-7b\",\n",
    "#     device_map=\"auto\",  # Automatically loads the model in the GPU, if there is one. Requires pip install acelerate\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.bfloat16   # If your GPU does not support bfloat16, change to torch.float16\n",
    "# )  \n",
    "\n",
    "# prompt = \"\"\"Classifique a resenha de filme como \"positiva\" ou \"negativa\".\n",
    "\n",
    "# Resenha: Gostei muito do filme, √© o melhor do ano!\n",
    "# Classe: positiva\n",
    "\n",
    "# Resenha: O filme deixa muito a desejar.\n",
    "# Classe: negativa\n",
    "\n",
    "# Resenha: Apesar de longo, valeu o ingresso.\n",
    "# Classe:\"\"\"\n",
    "\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# output = model.generate(\n",
    "#     input_ids[\"input_ids\"].to(\"cuda\"),\n",
    "#     max_length=1024,\n",
    "#     eos_token_id=tokenizer.encode(\"\\n\"))  # Stop generation when a \"\\n\" token is dectected\n",
    "\n",
    "# # The output contains the input tokens, so we have to skip them.\n",
    "# output = output[0][len(input_ids[\"input_ids\"][0]):]\n",
    "\n",
    "# print(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:22<00:00, 11.16s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandgibaut/periquito-3B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwandgibaut/periquito-3B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Troca para bfloat16\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\big_modeling.py:456\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."
     ]
    }
   ],
   "source": [
    "# Carregando o tokenizer e o modelo pr√©-treinado\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"wandgibaut/periquito-3B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"wandgibaut/periquito-3B\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,  # Troca para bfloat16\n",
    "    low_cpu_mem_usage=True\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"Giuliano/llama-2-7b-periquito-pt-br\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Giuliano/llama-2-7b-periquito-pt-br\",\n",
    "#     device_map=\"auto\",\n",
    "#     offload_folder=\"./offload\",  # Move partes do modelo para o disco\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Carregando o tokenizer e o modelo pr√©-treinado\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(\"maritaca-ai/sabia-2-tokenizer-small\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     \"maritaca-ai/sabia-2-tokenizer-small\",\n",
    "#     device_map=\"auto\",\n",
    "#     offload_folder=\"./offload\",  # Move partes do modelo para o disco\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.float16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3726/3726 [00:09<00:00, 401.45 examples/s]\n",
      "c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\cibel\\AppData\\Local\\Temp\\ipykernel_19904\\1586421316.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Criando o dataset compat√≠vel com Hugging Face\n",
    "train_data = Dataset.from_pandas(df_input_output[['input', 'output']])\n",
    "\n",
    "# Tokenizando os dados\n",
    "def preprocess_data(batch):\n",
    "    inputs = tokenizer(batch['input'], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    outputs = tokenizer(batch['output'], max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Para o modelo de causal LM, deslocamos os labels\n",
    "    labels = outputs['input_ids']\n",
    "    inputs['labels'] = labels\n",
    "    return inputs\n",
    "\n",
    "tokenized_data = train_data.map(preprocess_data, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "\n",
    "# **Treinando o modelo**\n",
    "# Configura√ß√£o de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\", #\"epoch\"\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,  # Defina como True se desejar fazer upload para o Hugging Face Hub\n",
    "    fp16=True  # Se a GPU suportar\n",
    ")\n",
    "\n",
    "# Criando o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5589 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 21.22 GiB is allocated by PyTorch, and 1.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Iniciando o treinamento\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3585\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\utils\\operations.py:820\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\utils\\operations.py:808\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[0;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    935\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         position_embeddings,\n\u001b[0;32m    943\u001b[0m     )\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:676\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    675\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 676\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    689\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:602\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m causal_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m q_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    611\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    612\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 21.22 GiB is allocated by PyTorch, and 1.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Iniciando o treinamento\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo treinado\n",
    "model.save_pretrained(\"./trained_model_maritaca_v1\")\n",
    "tokenizer.save_pretrained(\"./trained_model_maritaca_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar o modelo e o tokenizer ajustados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "def testar_modelo(prompt, modelo_caminho=\"./trained_model\", max_length=512, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    M√©todo para testar o modelo treinado.\n",
    "    \n",
    "    Par√¢metros:\n",
    "        - prompt (str): O texto de entrada para o modelo.\n",
    "        - modelo_caminho (str): Caminho para o modelo treinado salvo.\n",
    "        - max_length (int): Comprimento m√°ximo da gera√ß√£o.\n",
    "        - device (str): Dispositivo a ser usado ('cuda' para GPU, 'cpu' para CPU).\n",
    "    \n",
    "    Retorno:\n",
    "        - output_text (str): O texto gerado pelo modelo.\n",
    "    \"\"\"\n",
    "    # Carregar o modelo treinado e o tokenizer\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(modelo_caminho)\n",
    "    model = LlamaForCausalLM.from_pretrained(modelo_caminho)\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenizar o prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Gerar texto com o modelo\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        num_return_sequences=1,  # Gerar apenas uma sa√≠da\n",
    "        temperature=0.7,         # Controlar aleatoriedade\n",
    "        top_k=50                 # Limitar escolhas de palavras\n",
    "    )\n",
    "\n",
    "    # Decodificar o texto gerado\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Retornar apenas a parte gerada\n",
    "    return output_text[len(prompt):].strip()\n",
    "\n",
    "# Exemplo de uso\n",
    "prompt_teste = \"\"\"Tema: Tecnologia na educa√ß√£o\n",
    "Reda√ß√£o: A tecnologia tem transformado a maneira como aprendemos. Desde o uso de computadores em sala de aula at√© ferramentas como intelig√™ncia artificial, os avan√ßos tecnol√≥gicos t√™m ampliado o acesso √† educa√ß√£o. Contudo, √© necess√°rio que haja um equil√≠brio entre inova√ß√£o e inclus√£o digital.\n",
    "\n",
    "Corre√ß√£o:\"\"\"\n",
    "\n",
    "resultado = testar_modelo(prompt_teste)\n",
    "print(\"Sa√≠da do modelo:\", resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar o modelo e o tokenizer ajustados\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./modelo_finetuned_enem_v3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./modelo_finetuned_enem_v3\")\n",
    "\n",
    "# Definir o dispositivo (GPU se dispon√≠vel)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fun√ß√£o para avaliar reda√ß√£o\n",
    "# def avaliar_redacao(redacao, tema, max_length = 750):\n",
    "#     \"\"\"\n",
    "#     Avalia uma reda√ß√£o baseada em um tema usando um modelo fine-tuned GPT-like.\n",
    "\n",
    "#     Args:\n",
    "#         redacao (str): O texto da reda√ß√£o.\n",
    "#         tema (str): O tema da reda√ß√£o.\n",
    "#         max_length (int): Comprimento m√°ximo da resposta gerada.\n",
    "\n",
    "#     Returns:\n",
    "#         str: Resposta gerada pelo modelo.\n",
    "#     \"\"\"\n",
    "#     # Criar o prompt com a reda√ß√£o e o tema\n",
    "#     prompt_input = f\"\"\"\n",
    "#     Voc√™ √© um estudante do ensino m√©dio que est√° prestando vestibular para o ENEM, o Exame Nacional do Ensino M√©dio.\n",
    "#     Voc√™ deve se atentar √†:\n",
    "#     - Dom√≠nio da norma-padr√£o da l√≠ngua escrita;\n",
    "#     - Compreens√£o do tema;\n",
    "#     - Organiza√ß√£o das informa√ß√µes e argumentos;\n",
    "#     - Correta aplica√ß√£o da l√≥gica;\n",
    "#     - Apresentar uma proposta de interven√ß√£o para o problema.\n",
    "\n",
    "#     O tema da reda√ß√£o neste ano √© \"{tema}\".\n",
    "#     E voc√™ escreveu a seguinte reda√ß√£o:\n",
    "\n",
    "#     {redacao}\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Tokenizar o prompt usando o tokenizer e mover os tensores para o dispositivo correto\n",
    "#     inputs = tokenizer(\n",
    "#         prompt_input,\n",
    "#         return_tensors=\"pt\",  # Gera tensores PyTorch\n",
    "#         truncation=True # Limita o comprimento do prompt\n",
    "#     )\n",
    "#     input_ids = inputs[\"input_ids\"].to(device)\n",
    "#     attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "#     # Gerar a sa√≠da do modelo\n",
    "#     outputs = model.generate(\n",
    "#         input_ids=input_ids,  # IDs tokenizados\n",
    "#         attention_mask=attention_mask,  # M√°scara de aten√ß√£o\n",
    "#         max_length=max_length,  # Comprimento m√°ximo do texto gerado\n",
    "#         num_return_sequences=1,  # Gera apenas uma sequ√™ncia\n",
    "#         do_sample=True,  # Sampling para maior diversidade\n",
    "#         top_p=0.95,  # Nucleus sampling\n",
    "#         top_k=50  # Limita o n√∫mero de tokens considerados na gera√ß√£o\n",
    "#     )\n",
    "    \n",
    "#     # Decodificar a resposta do modelo\n",
    "#     resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "#     return resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_redacao(redacao, tema, max_length = 750):\n",
    "    # Criar o prompt com a reda√ß√£o e o tema\n",
    "    \n",
    "    prompt_input_ = \"\"\"\n",
    "        Voc√™ √© um estudante do ensino m√©dio que est√° prestando vestibular para o ENEM, o Exame Nacional do Ensino M√©dio. \n",
    "        Voc√™ deve se atentar √† \n",
    "        - Dom√≠nio da norma-padr√£o da l√≠ngua escrita; \n",
    "        - Compreens√£o do tema; \n",
    "        - Organiza√ß√£o das informa√ß√µes e argumentos;\n",
    "        - Correta aplica√ß√£o da l√≥gica;\n",
    "        - Apresentar uma proposta de interven√ß√£o para o problema.\n",
    "        \n",
    "        O tema da reda√ß√£ao neste ano √© \"{}\". \n",
    "        E voc√™ escreveu a seguinte reda√ß√£o:\n",
    "\n",
    "        {}\n",
    "    \"\"\".format(tema, redacao)\n",
    "    \n",
    "    prompt_input = \"Oi, tudo bem?\"\n",
    "\n",
    "    # Tokenizar o prompt\n",
    "    inputs = tokenizer(prompt_input, \n",
    "                            return_tensors=\"pt\"\n",
    "                            ).to(device)\n",
    "    #inputs = tokenizer(prompt, return_tensors=\"pt\").to(device) - # PT √© de temnsorflow = TODO: Usar modelos, maritaca, sabias, caramelo\n",
    "    \n",
    "    # Gerar a sa√≠da do modelo\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, num_return_sequences=1, do_sample=True, top_p=0.95, top_k=50)\n",
    "    \n",
    "    # Decodificar a resposta do modelo\n",
    "    resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oi, tudo bem? se*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".**teen?)*teen?\".*teen?\".teen?\".*teen?\".teen?\".*teen?\".teen?\".*teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de uso\n",
    "redacao_exemplo = \"\"\"\n",
    "A imigra√ß√£o de refugiado tem sido um dos maiores desafios das na√ß√µes. Este transpassa a quest√£o humanit√°ria e vai \n",
    "de encontro a  soberania de muitos pa√≠ses. N√£o diferente, O  Brasil enfrenta, como os demais  situa√ß√£o parecida. \n",
    "Nos √∫ltimos anos uma grande quantidade de refugiados t√™m transpassada  a fronteira do pa√≠s for√ßando o poder \n",
    "p√∫blico a tomar medidas que garantam a integridade da soberania nacional e que esteja em conson√¢ncia aos direitos humanos.  \n",
    "V√°rias na√ß√µes do mundo est√£o passando por percal√ßos, seja por  guerras, crises econ√¥micas, conflitos internos, regimes ditatoriais... \n",
    "Fato que  faz surgir refugiados que vislumbram nos pa√≠ses mais pr√≥ximos a salva guarda  para manterem a integridade. O que para estes  \n",
    "√© um alento, para o pa√≠s em que v√£o se refugiam  se torna um problema. Que muitas vezes vezes,  n√£o est√° relacionado diretamente \n",
    "a um sentimento xen√≥fobo, mas sim por  uma quest√£o social. No caso do Brasil, os refugiados da nossa na√ß√£o adjacente se abrigaram \n",
    "em uma cidade de um estado da federa√ß√£o  que tem grande dificuldade para cuidar at√© mesmo dos nativos. Causando  um verdadeiro caos. \n",
    "O que  fez aumentar ainda mais o sofrimento de quem j√° est√° em uma situa√ß√£o extrema. Destarte que  n√£o basta o mero acolhimento, \n",
    "deve-se socorrer e fomentar uma nova expectativa de vida nestas  pessoas que est√£o distantes do pa√≠s de origem. O que  tende a \n",
    "demandar tempo e recursos. No caso brasileiro  um disp√™ndio a mais em um pa√≠s que est√° em busca de reduzir despesas. Salientando \n",
    "que se faz necess√°rio uma previa  triagem para evitar que pessoas criminosas e mal-intencionadas venham fazer da na√ß√£o um local \n",
    "para praticas criminosa  e potencialize a viol√™ncia. Desprende  que o maior desafio no trato com os imigrantes est√° em conciliar \n",
    "a soberana for√ßa nacional de prote√ß√£o com a aplica√ß√£o de direitos humanos. √â impens√°vel deixar de ajudar o semelhante em apuros. \n",
    "Efetivar esse apoio sem prejudicar os demais √© tarefa que requer recursos e planejamentos. O que aos poucos o pa√≠s vai se consolidando.\n",
    "\n",
    "\"\"\"\n",
    "tema_exemplo = \"Desafios na imigra√ß√£o de refugiados\"\n",
    "\n",
    "correcao = avaliar_redacao(redacao_exemplo, tema_exemplo)\n",
    "print(correcao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
