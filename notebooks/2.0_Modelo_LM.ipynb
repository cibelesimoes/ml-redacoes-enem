{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch\n",
    "# !pip install transformers[torch]\n",
    "#!pip uninstall numpy\n",
    "#!pip install numpy==1.26.4\n",
    "#!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelWithLMHead, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Deve retornar True\n",
    "print(torch.cuda.get_device_name(0))  # Nome da sua GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando GPU: NVIDIA GeForce RTX 4060\n",
      "tensor([[ 1.5596, -0.9572,  0.0136],\n",
      "        [ 0.2888, -1.1967,  2.0747],\n",
      "        [ 1.9184,  0.4666,  0.4987]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Verificar se a GPU está disponível\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Define a GPU como dispositivo\n",
    "    print(f\"Usando GPU: {torch.cuda.get_device_name(0)}\")  # Imprime o nome da GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Se não houver GPU disponível, usa a CPU\n",
    "    print(\"Usando CPU\")\n",
    "\n",
    "# Exemplo de como mover um tensor para a GPU\n",
    "tensor = torch.randn(3, 3)  # Tensor aleatório\n",
    "tensor = tensor.to(device)  # Mover para o dispositivo correto (GPU ou CPU)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['nvidia-smi'], returncode=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Verificar o uso da GPU\n",
    "subprocess.run([\"nvidia-smi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tema</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>comments</th>\n",
       "      <th>criteria_score_1</th>\n",
       "      <th>criteria_score_2</th>\n",
       "      <th>criteria_score_3</th>\n",
       "      <th>criteria_score_4</th>\n",
       "      <th>criteria_score_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Os ursos polares da Rússia e um dilema ecológico</td>\n",
       "      <td>As mudanças que afetam o clima de nosso planet...</td>\n",
       "      <td>Natureza caótica</td>\n",
       "      <td>A natureza possui um equilíbrio singular singu...</td>\n",
       "      <td>Texto muito bom, com alguns erros e equívocos ...</td>\n",
       "      <td>160</td>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Os ursos polares da Rússia e um dilema ecológico</td>\n",
       "      <td>As mudanças que afetam o clima de nosso planet...</td>\n",
       "      <td>Em busca de um novo lar</td>\n",
       "      <td>Com o aumento do aquecimento global, várias es...</td>\n",
       "      <td>Texto mediano. Além dos problemas pontuais neg...</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Os ursos polares da Rússia e um dilema ecológico</td>\n",
       "      <td>As mudanças que afetam o clima de nosso planet...</td>\n",
       "      <td>A ação antrópica e suas consequências</td>\n",
       "      <td>Há, na pré-história, pinturas rupestres que re...</td>\n",
       "      <td>Texto prolixo e repetitivo, com problemas pont...</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Os ursos polares da Rússia e um dilema ecológico</td>\n",
       "      <td>As mudanças que afetam o clima de nosso planet...</td>\n",
       "      <td>O aquecimento global e os ursos</td>\n",
       "      <td>O aquecimento global vem afetando o mundo nos ...</td>\n",
       "      <td>Lamentavelmente, o texto é fraco, insuficiente...</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Os ursos polares da Rússia e um dilema ecológico</td>\n",
       "      <td>As mudanças que afetam o clima de nosso planet...</td>\n",
       "      <td>As consequências do degelo no Ártico</td>\n",
       "      <td>O aumento do aquecimento global provocou, no í...</td>\n",
       "      <td>Texto mediano. O autor cumpre as exigências da...</td>\n",
       "      <td>80</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tema  \\\n",
       "0  Os ursos polares da Rússia e um dilema ecológico   \n",
       "1  Os ursos polares da Rússia e um dilema ecológico   \n",
       "2  Os ursos polares da Rússia e um dilema ecológico   \n",
       "3  Os ursos polares da Rússia e um dilema ecológico   \n",
       "4  Os ursos polares da Rússia e um dilema ecológico   \n",
       "\n",
       "                                         description  \\\n",
       "0  As mudanças que afetam o clima de nosso planet...   \n",
       "1  As mudanças que afetam o clima de nosso planet...   \n",
       "2  As mudanças que afetam o clima de nosso planet...   \n",
       "3  As mudanças que afetam o clima de nosso planet...   \n",
       "4  As mudanças que afetam o clima de nosso planet...   \n",
       "\n",
       "                                   title  \\\n",
       "0                       Natureza caótica   \n",
       "1                Em busca de um novo lar   \n",
       "2  A ação antrópica e suas consequências   \n",
       "3        O aquecimento global e os ursos   \n",
       "4   As consequências do degelo no Ártico   \n",
       "\n",
       "                                                text  \\\n",
       "0  A natureza possui um equilíbrio singular singu...   \n",
       "1  Com o aumento do aquecimento global, várias es...   \n",
       "2  Há, na pré-história, pinturas rupestres que re...   \n",
       "3  O aquecimento global vem afetando o mundo nos ...   \n",
       "4  O aumento do aquecimento global provocou, no í...   \n",
       "\n",
       "                                            comments  criteria_score_1  \\\n",
       "0  Texto muito bom, com alguns erros e equívocos ...               160   \n",
       "1  Texto mediano. Além dos problemas pontuais neg...               120   \n",
       "2  Texto prolixo e repetitivo, com problemas pont...               120   \n",
       "3  Lamentavelmente, o texto é fraco, insuficiente...               120   \n",
       "4  Texto mediano. O autor cumpre as exigências da...                80   \n",
       "\n",
       "   criteria_score_2  criteria_score_3  criteria_score_4  criteria_score_5  \n",
       "0               200               160               200               160  \n",
       "1               120               120               120               120  \n",
       "2               120                80               120               120  \n",
       "3                80                80                80                80  \n",
       "4               120               120               120               120  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caminho_redacoes_corrigidas = os.path.join('..', 'data', 'redacoes_corrigidas_uol_fora_enem.csv')\n",
    "\n",
    "df_redacoes_corrigidas = pd.read_csv(caminho_redacoes_corrigidas, sep=\";\")\n",
    "df_redacoes_corrigidas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redacoes_corrigidas[\"nota_final\"] = df_redacoes_corrigidas[\"criteria_score_1\"] + df_redacoes_corrigidas[\"criteria_score_2\"] + df_redacoes_corrigidas[\"criteria_score_3\"] + df_redacoes_corrigidas[\"criteria_score_4\"] + df_redacoes_corrigidas[\"criteria_score_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Os ursos polares da Rússia e um dilema ecológico'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_redacoes_corrigidas['tema'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Você é um estudante do ensino médio que está prestando vestibular para o ENEM, o Exame Nacional do Ensino Médio. \n",
      "    Você deve se atentar à \n",
      "     - Domínio da norma-padrão da língua escrita; \n",
      "     - Compreensão do tema; \n",
      "     - Organização das informações e argumentos;\n",
      "     - Correta aplicação da lógica;\n",
      "    - Apresentar uma proposta de intervenção para o problema.\n",
      "    \n",
      "    O tema da redaçãao neste ano é \"Os ursos polares da Rússia e um dilema ecológico\". \n",
      "    E você escreveu a seguinte redação:\n",
      "\n",
      "    A natureza possui um equilíbrio singular singular, de modo que todos os seres vivos desempenham uma função - os ursos polares, por exemplo, ao se alimentarem de peixes e focas focas, regulam a quantidade de indivíduos nessas populações. Contudo, os homens ainda não compreendem a importância do respeito ao meio ambiente para a preservação da própria espécie e, por isso, têm arcado com as consequências, como o ataque dos ursos polares a comunidades humanas por diminuição da oferta de alimentos, devido ao derretimento das geleiras. A preocupação com a preservação ambiental tornou-se uma pauta mais frequente urgente, a partir da década de 70, desde então 70. Desde então, vários acordos para diminuir as queimadas, o desmatamento e, principalmente, a emissão de gases poluentes foram assinados, mas não atingiram o efeito esperado, pois demandam uma mudança radical na economia, que parte dos países não está disposta a fazer. Como o Estados Unidos, segundo maior poluidor do mundo, que não participou do Tratado de Kyoto e se retirou do Acordo de Paris devido ao comprometimento econômico que eles acarretariam. Um dos impasses resultantes das mudanças climáticas é a maneira de lidar com animais que invadem as cidades, já que é necessário um cuidado maior para que as pessoas e os animais não se machuquem. O caso dos ursos polares é preocupante nesse aspecto, pois eles são violentos e precisam ser contidos depressa, mas, como estão em extinção, não podem ser mortos e, mesmo que pudessem, isso não resolveria o problema e ainda originaria outros desequilíbrios. Para efetivamente saná-lo, deve-se mitigar a causa, que é o aquecimento global através da redução do uso de combustíveis fosseis fósseis. Enquanto não há priorização desse assunto nas ações governamentais, a natureza e a sociedade, em geral, continuam sofrendo. É por essa razão que a população mundial precisa se mobilizar e valorizar politicas políticas sustentáveis dado que o planeta é responsabilidade de todos e adiar a sua preservação pode trazer resultados irreparáveis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Passo 2: Pré-processar os dados\n",
    "# Unir a redação e correção no formato de entrada do modelo\n",
    "\n",
    "prompt_input = \"\"\"\n",
    "    Você é um estudante do ensino médio que está prestando vestibular para o ENEM, o Exame Nacional do Ensino Médio. \n",
    "    Você deve se atentar à \n",
    "     - Domínio da norma-padrão da língua escrita; \n",
    "     - Compreensão do tema; \n",
    "     - Organização das informações e argumentos;\n",
    "     - Correta aplicação da lógica;\n",
    "    - Apresentar uma proposta de intervenção para o problema.\n",
    "    \n",
    "    O tema da redaçãao neste ano é \"{}\". \n",
    "    E você escreveu a seguinte redação:\n",
    "\n",
    "    {}\n",
    "\"\"\".format(df_redacoes_corrigidas['tema'][0],  df_redacoes_corrigidas['text'][0])\n",
    "\n",
    "print(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   A correção da sua redação foi a seguinte:\n",
      "   Texto muito bom, com alguns erros e equívocos em termos de conteúdo, que impedem a atribuição de uma nota ainda melhor. Parágrafo 1) Texto muito bom em termos de linguagem. Os erros gramaticais apontados em verde impedem que se lhe atribua a nota máxima. Destaque-se que o título não tem nada que ver com o texto. Parágrafo 2) O autor compreendeu o tema e desenvolveu um texto dissertativo-argumentativo obedecendo as exigências desta competência. Parágrafo 3) Argumentação boa. O autor contextualiza bem o problema. Só se equivoca ao dizer que os homens não compreendem a necessidade de respeitar o meio ambiente. A afirmação é excessivamente genérica e contraditória, pois, se vários acordos e protocolos foram assinados desde os anos 70, uma parte significativa das nações se mostra preocupada com o problema. Também é estranho argumentar com o \"segundo maior poluidor\". Seria melhor apontar o primeiro maior ou os dois países. Parágrafo 4) A argumentação está bem construída e o texto é coerente e coeso. Parágrafo 5) A sugestão é genérica e está voltada para o problema mais amplo do aquecimento global. Como ficam os ursos?. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_output = \"\"\"\n",
    "   A correção da sua redação foi a seguinte:\n",
    "   {}. \n",
    "\"\"\".format(df_redacoes_corrigidas['comments'][0])\n",
    "\n",
    "print(prompt_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_output = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output'],\n",
       "    num_rows: 3726\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input_output['input'] = df_redacoes_corrigidas.apply(\n",
    "    lambda x: prompt_input.format(x['tema'], x['text']), axis=1\n",
    ")\n",
    "df_input_output['output'] = df_redacoes_corrigidas.apply(\n",
    "    lambda x: prompt_output.format(x['comments']), axis=1\n",
    ")\n",
    "\n",
    "train_data = Dataset.from_pandas(df_input_output[['input', 'output']])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3726/3726 [00:01<00:00, 2708.24 examples/s]\n",
      "C:\\Users\\cibel\\AppData\\Local\\Temp\\ipykernel_19864\\3098190202.py:55: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Passo 4: Tokenização das entradas e saídas\n",
    "max_length = 750\n",
    "\n",
    "# Carregar o tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pierreguillou/gpt2-small-portuguese\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Definir o token de pad\n",
    "\n",
    "# Função de tokenização\n",
    "def tokenize_function(examples):\n",
    "    # Tokenizando as entradas\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['input'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Definir os labels como os mesmos tokens da entrada\n",
    "    # O modelo GPT2 espera que os labels sejam deslocados em relação à entrada\n",
    "    labels = tokenized_inputs[\"input_ids\"].clone()\n",
    "\n",
    "    # Para treinamento causal, deslocar os labels para a direita\n",
    "    labels[:, :-1] = labels[:, 1:].clone()\n",
    "    labels[:, -1] = -100  # Prevenir que o último token da sequência seja considerado para o cálculo da perda\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Criar o dataset tokenizado\n",
    "tokenized_dataset = train_data.map(\n",
    "    tokenize_function, \n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Passo 5: Configuração do treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_enem\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs_enem\",\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=4,\n",
    "    gradient_accumulation_steps=4,\n",
    ")\n",
    "\n",
    "# Passo 6: Carregar o modelo GPT-2\n",
    "model = AutoModelForCausalLM.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
    "\n",
    "# Passo 7: Inicializar o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,  # Passar o tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/232 [00:00<?, ?it/s]c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 22%|██▏       | 50/232 [15:04<42:09, 13.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 18.8628, 'grad_norm': 5.6947245597839355, 'learning_rate': 4.245689655172414e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 43%|████▎     | 100/232 [27:59<44:29, 20.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8986, 'grad_norm': 1.577386498451233, 'learning_rate': 3.2112068965517244e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 65%|██████▍   | 150/232 [46:17<30:53, 22.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2519, 'grad_norm': 1.1526674032211304, 'learning_rate': 2.1551724137931033e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 86%|████████▌ | 200/232 [1:04:44<11:23, 21.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1875, 'grad_norm': 1.3092814683914185, 'learning_rate': 1.0991379310344827e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cibel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "100%|██████████| 232/232 [1:16:28<00:00, 19.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4588.6703, 'train_samples_per_second': 1.624, 'train_steps_per_second': 0.051, 'train_loss': 4.383988507862749, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=232, training_loss=4.383988507862749, metrics={'train_runtime': 4588.6703, 'train_samples_per_second': 1.624, 'train_steps_per_second': 0.051, 'total_flos': 2840785344000000.0, 'train_loss': 4.383988507862749, 'epoch': 1.9914163090128756})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Passo 8: Iniciar o treinamento\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./modelo_finetuned_enem_v3\\\\tokenizer_config.json',\n",
       " './modelo_finetuned_enem_v3\\\\special_tokens_map.json',\n",
       " './modelo_finetuned_enem_v3\\\\vocab.json',\n",
       " './modelo_finetuned_enem_v3\\\\merges.txt',\n",
       " './modelo_finetuned_enem_v3\\\\added_tokens.json',\n",
       " './modelo_finetuned_enem_v3\\\\tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passo 8: Salvar o modelo e o tokenizer\n",
    "model.save_pretrained(\"./modelo_finetuned_enem_v3\")\n",
    "tokenizer.save_pretrained(\"./modelo_finetuned_enem_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar o modelo e o tokenizer ajustados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar o modelo e o tokenizer ajustados\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./modelo_finetuned_enem_v3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./modelo_finetuned_enem_v3\")\n",
    "\n",
    "# Definir o dispositivo (GPU se disponível)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Função para avaliar redação\n",
    "# def avaliar_redacao(redacao, tema, max_length = 750):\n",
    "#     \"\"\"\n",
    "#     Avalia uma redação baseada em um tema usando um modelo fine-tuned GPT-like.\n",
    "\n",
    "#     Args:\n",
    "#         redacao (str): O texto da redação.\n",
    "#         tema (str): O tema da redação.\n",
    "#         max_length (int): Comprimento máximo da resposta gerada.\n",
    "\n",
    "#     Returns:\n",
    "#         str: Resposta gerada pelo modelo.\n",
    "#     \"\"\"\n",
    "#     # Criar o prompt com a redação e o tema\n",
    "#     prompt_input = f\"\"\"\n",
    "#     Você é um estudante do ensino médio que está prestando vestibular para o ENEM, o Exame Nacional do Ensino Médio.\n",
    "#     Você deve se atentar à:\n",
    "#     - Domínio da norma-padrão da língua escrita;\n",
    "#     - Compreensão do tema;\n",
    "#     - Organização das informações e argumentos;\n",
    "#     - Correta aplicação da lógica;\n",
    "#     - Apresentar uma proposta de intervenção para o problema.\n",
    "\n",
    "#     O tema da redação neste ano é \"{tema}\".\n",
    "#     E você escreveu a seguinte redação:\n",
    "\n",
    "#     {redacao}\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Tokenizar o prompt usando o tokenizer e mover os tensores para o dispositivo correto\n",
    "#     inputs = tokenizer(\n",
    "#         prompt_input,\n",
    "#         return_tensors=\"pt\",  # Gera tensores PyTorch\n",
    "#         truncation=True # Limita o comprimento do prompt\n",
    "#     )\n",
    "#     input_ids = inputs[\"input_ids\"].to(device)\n",
    "#     attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "#     # Gerar a saída do modelo\n",
    "#     outputs = model.generate(\n",
    "#         input_ids=input_ids,  # IDs tokenizados\n",
    "#         attention_mask=attention_mask,  # Máscara de atenção\n",
    "#         max_length=max_length,  # Comprimento máximo do texto gerado\n",
    "#         num_return_sequences=1,  # Gera apenas uma sequência\n",
    "#         do_sample=True,  # Sampling para maior diversidade\n",
    "#         top_p=0.95,  # Nucleus sampling\n",
    "#         top_k=50  # Limita o número de tokens considerados na geração\n",
    "#     )\n",
    "    \n",
    "#     # Decodificar a resposta do modelo\n",
    "#     resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "#     return resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_redacao(redacao, tema, max_length = 750):\n",
    "    # Criar o prompt com a redação e o tema\n",
    "    \n",
    "    prompt_input_ = \"\"\"\n",
    "        Você é um estudante do ensino médio que está prestando vestibular para o ENEM, o Exame Nacional do Ensino Médio. \n",
    "        Você deve se atentar à \n",
    "        - Domínio da norma-padrão da língua escrita; \n",
    "        - Compreensão do tema; \n",
    "        - Organização das informações e argumentos;\n",
    "        - Correta aplicação da lógica;\n",
    "        - Apresentar uma proposta de intervenção para o problema.\n",
    "        \n",
    "        O tema da redaçãao neste ano é \"{}\". \n",
    "        E você escreveu a seguinte redação:\n",
    "\n",
    "        {}\n",
    "    \"\"\".format(tema, redacao)\n",
    "    \n",
    "    prompt_input = \"Oi, tudo bem?\"\n",
    "\n",
    "    # Tokenizar o prompt\n",
    "    inputs = tokenizer(prompt_input, \n",
    "                            return_tensors=\"pt\"\n",
    "                            ).to(device)\n",
    "    #inputs = tokenizer(prompt, return_tensors=\"pt\").to(device) - # PT é de temnsorflow = TODO: Usar modelos, maritaca, sabias, caramelo\n",
    "    \n",
    "    # Gerar a saída do modelo\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, num_return_sequences=1, do_sample=True, top_p=0.95, top_k=50)\n",
    "    \n",
    "    # Decodificar a resposta do modelo\n",
    "    resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oi, tudo bem? se*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?)*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".*teen?\".**teen?)*teen?\".*teen?\".teen?\".*teen?\".teen?\".*teen?\".teen?\".*teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen?\".teen\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de uso\n",
    "redacao_exemplo = \"\"\"\n",
    "A imigração de refugiado tem sido um dos maiores desafios das nações. Este transpassa a questão humanitária e vai \n",
    "de encontro a  soberania de muitos países. Não diferente, O  Brasil enfrenta, como os demais  situação parecida. \n",
    "Nos últimos anos uma grande quantidade de refugiados têm transpassada  a fronteira do país forçando o poder \n",
    "público a tomar medidas que garantam a integridade da soberania nacional e que esteja em consonância aos direitos humanos.  \n",
    "Várias nações do mundo estão passando por percalços, seja por  guerras, crises econômicas, conflitos internos, regimes ditatoriais... \n",
    "Fato que  faz surgir refugiados que vislumbram nos países mais próximos a salva guarda  para manterem a integridade. O que para estes  \n",
    "é um alento, para o país em que vão se refugiam  se torna um problema. Que muitas vezes vezes,  não está relacionado diretamente \n",
    "a um sentimento xenófobo, mas sim por  uma questão social. No caso do Brasil, os refugiados da nossa nação adjacente se abrigaram \n",
    "em uma cidade de um estado da federação  que tem grande dificuldade para cuidar até mesmo dos nativos. Causando  um verdadeiro caos. \n",
    "O que  fez aumentar ainda mais o sofrimento de quem já está em uma situação extrema. Destarte que  não basta o mero acolhimento, \n",
    "deve-se socorrer e fomentar uma nova expectativa de vida nestas  pessoas que estão distantes do país de origem. O que  tende a \n",
    "demandar tempo e recursos. No caso brasileiro  um dispêndio a mais em um país que está em busca de reduzir despesas. Salientando \n",
    "que se faz necessário uma previa  triagem para evitar que pessoas criminosas e mal-intencionadas venham fazer da nação um local \n",
    "para praticas criminosa  e potencialize a violência. Desprende  que o maior desafio no trato com os imigrantes está em conciliar \n",
    "a soberana força nacional de proteção com a aplicação de direitos humanos. É impensável deixar de ajudar o semelhante em apuros. \n",
    "Efetivar esse apoio sem prejudicar os demais é tarefa que requer recursos e planejamentos. O que aos poucos o país vai se consolidando.\n",
    "\n",
    "\"\"\"\n",
    "tema_exemplo = \"Desafios na imigração de refugiados\"\n",
    "\n",
    "correcao = avaliar_redacao(redacao_exemplo, tema_exemplo)\n",
    "print(correcao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
